<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">39409216</PMID><DateCompleted><Year>2024</Year><Month>10</Month><Day>16</Day></DateCompleted><DateRevised><Year>2024</Year><Month>10</Month><Day>19</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">1424-8220</ISSN><JournalIssue CitedMedium="Internet"><Volume>24</Volume><Issue>19</Issue><PubDate><Year>2024</Year><Month>Sep</Month><Day>24</Day></PubDate></JournalIssue><Title>Sensors (Basel, Switzerland)</Title><ISOAbbreviation>Sensors (Basel)</ISOAbbreviation></Journal><ArticleTitle>Fused Audio Instance and Representation for Respiratory Disease Detection.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">6176</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/s24196176</ELocationID><Abstract><AbstractText>Audio-based classification techniques for body sounds have long been studied to aid in the diagnosis of respiratory diseases. While most research is centered on the use of coughs as the main acoustic biomarker, other body sounds also have the potential to detect respiratory diseases. Recent studies on the coronavirus disease 2019 (COVID-19) have suggested that breath and speech sounds, in addition to cough, correlate with the disease. Our study proposes fused audio instance and representation (FAIR) as a method for respiratory disease detection. FAIR relies on constructing a joint feature vector from various body sounds represented in waveform and spectrogram form. We conduct experiments on the use case of COVID-19 detection by combining waveform and spectrogram representation of body sounds. Our findings show that the use of self-attention to combine extracted features from cough, breath, and speech sounds leads to the best performance with an area under the receiver operating characteristic curve (AUC) score of 0.8658, a sensitivity of 0.8057, and a specificity of 0.7958. Compared to models trained solely on spectrograms or waveforms, the use of both representations results in an improved AUC score, demonstrating that combining spectrogram and waveform representation helps to enrich the extracted features and outperforms the models that use only one representation. While this study focuses on COVID-19, FAIR's flexibility allows it to combine various multi-modal and multi-instance features in many other diagnostic applications, potentially leading to more accurate diagnoses across a wider range of diseases.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Truong</LastName><ForeName>Tuan</ForeName><Initials>T</Initials><Identifier Source="ORCID">0000-0003-0284-4269</Identifier><AffiliationInfo><Affiliation>Bayer AG, 13353 Berlin, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Lenga</LastName><ForeName>Matthias</ForeName><Initials>M</Initials><Identifier Source="ORCID">0000-0003-3771-2012</Identifier><AffiliationInfo><Affiliation>Bayer AG, 13353 Berlin, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Serrurier</LastName><ForeName>Antoine</ForeName><Initials>A</Initials><Identifier Source="ORCID">0000-0001-8268-6832</Identifier><AffiliationInfo><Affiliation>Clinic for Phoniatrics, Pedaudiology and Communication Disorders, University Hospital of RWTH Aachen, 52074 Aachen, Germany.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Mohammadi</LastName><ForeName>Sadegh</ForeName><Initials>S</Initials><AffiliationInfo><Affiliation>Bayer AG, 13353 Berlin, Germany.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2024</Year><Month>09</Month><Day>24</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Sensors (Basel)</MedlineTA><NlmUniqueID>101204366</NlmUniqueID><ISSNLinking>1424-8220</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName><QualifierName UI="Q000175" MajorTopicYN="N">diagnosis</QualifierName><QualifierName UI="Q000821" MajorTopicYN="N">virology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000086402" MajorTopicYN="Y">SARS-CoV-2</DescriptorName><QualifierName UI="Q000302" MajorTopicYN="N">isolation &amp; purification</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D013018" MajorTopicYN="N">Sound Spectrography</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D003371" MajorTopicYN="N">Cough</DescriptorName><QualifierName UI="Q000175" MajorTopicYN="N">diagnosis</QualifierName><QualifierName UI="Q000503" MajorTopicYN="N">physiopathology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D012135" MajorTopicYN="N">Respiratory Sounds</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D012372" MajorTopicYN="N">ROC Curve</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D012815" MajorTopicYN="N">Signal Processing, Computer-Assisted</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">audio</Keyword><Keyword MajorTopicYN="N">classification</Keyword><Keyword MajorTopicYN="N">deep learning</Keyword><Keyword MajorTopicYN="N">multi-instance learning</Keyword><Keyword MajorTopicYN="N">respiratory disease</Keyword><Keyword MajorTopicYN="N">spectrogram</Keyword><Keyword MajorTopicYN="N">waveform</Keyword></KeywordList><CoiStatement>The authors T.T., M.L., and S.M. were employed by Bayer AG. The remaining author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2024</Year><Month>8</Month><Day>19</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2024</Year><Month>9</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2024</Year><Month>9</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2024</Year><Month>10</Month><Day>16</Day><Hour>10</Hour><Minute>20</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2024</Year><Month>10</Month><Day>16</Day><Hour>10</Hour><Minute>19</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2024</Year><Month>10</Month><Day>16</Day><Hour>1</Hour><Minute>11</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>9</Month><Day>24</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">39409216</ArticleId><ArticleId IdType="pmc">PMC11479208</ArticleId><ArticleId IdType="doi">10.3390/s24196176</ArticleId><ArticleId IdType="pii">s24196176</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>De Meyer M.M., Jacquet W., Vanderveken O.M., Marks L.A. Systematic review of the different aspects of primary snoring. Sleep Med. Rev. 2019;45:88–94. doi: 10.1016/j.smrv.2019.03.001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.smrv.2019.03.001</ArticleId><ArticleId IdType="pubmed">30978609</ArticleId></ArticleIdList></Reference><Reference><Citation>Sarkar M., Madabhavi I., Niranjan N., Dogra M. Auscultation of the respiratory system. Ann. Thorac. Med. 2015;10:158. doi: 10.4103/1817-1737.160831.</Citation><ArticleIdList><ArticleId IdType="doi">10.4103/1817-1737.160831</ArticleId><ArticleId IdType="pmc">PMC4518345</ArticleId><ArticleId IdType="pubmed">26229557</ArticleId></ArticleIdList></Reference><Reference><Citation>Song I. Diagnosis of pneumonia from sounds collected using low cost cell phones; Proceedings of the 2015 International Joint Conference on Neural Networks (IJCNN); Killarney, Ireland. 12–17 July 2015; pp. 1–8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/IJCNN.2015.7280317</ArticleId></ArticleIdList></Reference><Reference><Citation>Laguarta J., Hueto F., Subirana B. COVID-19 Artificial Intelligence Diagnosis Using Only Cough Recordings. IEEE Open J. Eng. Med. Biol. 2020;1:275–281. doi: 10.1109/OJEMB.2020.3026928.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/OJEMB.2020.3026928</ArticleId><ArticleId IdType="pmc">PMC8545024</ArticleId><ArticleId IdType="pubmed">34812418</ArticleId></ArticleIdList></Reference><Reference><Citation>Botha G.H.R., Theron G., Warren R.M., Klopper M., Dheda K., van Helden P.D., Niesler T.R. Detection of tuberculosis by automatic cough sound analysis. Physiol. Meas. 2018;39:045005. doi: 10.1088/1361-6579/aab6d0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6579/aab6d0</ArticleId><ArticleId IdType="pubmed">29543189</ArticleId></ArticleIdList></Reference><Reference><Citation>Altan G., Kutlu Y., Allahverdi N. Deep Learning on Computerized Analysis of Chronic Obstructive Pulmonary Disease. IEEE J. Biomed. Health Inform. 2020;24:1344–1350. doi: 10.1109/JBHI.2019.2931395.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/JBHI.2019.2931395</ArticleId><ArticleId IdType="pubmed">31369388</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang H., Song C., Wang A., Xu C., Li D., Xu W. PDVocal: Towards Privacy-preserving Parkinson’s Disease Detection using Non-speech Body Sounds; Proceedings of the 25th Annual International Conference on Mobile Computing and Networking; Los Cabos, Mexico. 21–25 October 2019; pp. 1–16.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3300061.3300125</ArticleId></ArticleIdList></Reference><Reference><Citation>Kalkbrenner C., Eichenlaub M., Rüdiger S., Kropf-Sanchen C., Rottbauer W., Brucher R. Apnea and heart rate detection from tracheal body sounds for the diagnosis of sleep-related breathing disorders. Med Biol. Eng. Comput. 2018;56:671–681. doi: 10.1007/s11517-017-1706-y.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11517-017-1706-y</ArticleId><ArticleId IdType="pubmed">28849304</ArticleId></ArticleIdList></Reference><Reference><Citation>Astuti I., Ysrafil Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2): An overview of viral structure and host response. Diabetes Metab. Syndr. Clin. Res. Rev. 2020;14:407–412. doi: 10.1016/j.dsx.2020.04.020.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.dsx.2020.04.020</ArticleId><ArticleId IdType="pmc">PMC7165108</ArticleId><ArticleId IdType="pubmed">32335367</ArticleId></ArticleIdList></Reference><Reference><Citation>Scheiblauer H., Filomena A., Nitsche A., Puyskens A., Corman V.M., Drosten C., Zwirglmaier K., Lange C., Emmerich P., Müller M., et al. Comparative sensitivity evaluation for 122 CE-marked rapid diagnostic tests for SARS-CoV-2 antigen, Germany, September 2020 to April 2021. Eurosurveillance. 2021;26:2100441. doi: 10.2807/1560-7917.ES.2021.26.44.2100441.</Citation><ArticleIdList><ArticleId IdType="doi">10.2807/1560-7917.ES.2021.26.44.2100441</ArticleId><ArticleId IdType="pmc">PMC8569926</ArticleId><ArticleId IdType="pubmed">34738515</ArticleId></ArticleIdList></Reference><Reference><Citation>Huang Y., Meng S., Zhang Y., Wu S., Zhang Y., Zhang Y., Ye Y., Wei Q., Zhao N., Jiang J., et al. The respiratory sound features of COVID-19 patients fill gaps between clinical data and screening methods. medRxiv. 2020 doi: 10.1101/2020.04.07.20051060.</Citation><ArticleIdList><ArticleId IdType="doi">10.1101/2020.04.07.20051060</ArticleId></ArticleIdList></Reference><Reference><Citation>Al Ismail M., Deshmukh S., Singh R. Detection of Covid-19 Through the Analysis of Vocal Fold Oscillations; Proceedings of the ICASSP 2021—2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); Toronto, ON, Canada. 6–11 June 2021; pp. 1035–1039.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ICASSP39728.2021.9414201</ArticleId></ArticleIdList></Reference><Reference><Citation>Shimon C., Shafat G., Dangoor I., Ben-Shitrit A. Artificial intelligence enabled preliminary diagnosis for COVID-19 from voice cues and questionnaires. J. Acoust. Soc. Am. 2021;149:1120–1124. doi: 10.1121/10.0003434.</Citation><ArticleIdList><ArticleId IdType="doi">10.1121/10.0003434</ArticleId><ArticleId IdType="pmc">PMC7928231</ArticleId><ArticleId IdType="pubmed">33639822</ArticleId></ArticleIdList></Reference><Reference><Citation>Suppakitjanusant P., Sungkanuparph S., Wongsinin T., Virapongsiri S., Kasemkosin N., Chailurkit L., Ongphiphadhanakul B. Identifying individuals with recent COVID-19 through voice classification using deep learning. Sci. Rep. 2021;11:19149. doi: 10.1038/s41598-021-98742-x.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-021-98742-x</ArticleId><ArticleId IdType="pmc">PMC8476606</ArticleId><ArticleId IdType="pubmed">34580407</ArticleId></ArticleIdList></Reference><Reference><Citation>Pahar M., Klopper M., Reeve B., Warren R., Theron G., Niesler T. Automatic cough classification for tuberculosis screening in a real-world environment. Physiol. Meas. 2021;42:105014. doi: 10.1088/1361-6579/ac2fb8.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1361-6579/ac2fb8</ArticleId><ArticleId IdType="pmc">PMC8721487</ArticleId><ArticleId IdType="pubmed">34649231</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu X., Nemati E., Vatanparvar K., Nathan V., Ahmed T., Rahman M.M., McCaffrey D., Kuang J., Gao J.A. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. Volume 5. ACM; New York, NY, USA: 2021. Listen2Cough: Leveraging End-to-End Deep Learning Cough Detection Model to Enhance Lung Health Assessment Using Passively Sensed Audio; pp. 1–22.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3448124</ArticleId></ArticleIdList></Reference><Reference><Citation>Khanaghavalle G., Rahul G., Senajith S., Vishnuvasan T., Keerthana S. Chronic Obstructive Pulmonary Disease Severity Classification using lung Sound; Proceedings of the 2024 10th International Conference on Communication and Signal Processing (ICCSP); Melmaruvathur, India. 12–14 April 2024; pp. 428–432.</Citation></Reference><Reference><Citation>Luo K., Yang G., Li Y., Lan S., Wang Y., He L., Hu B. Croup and pertussis cough sound classification algorithm based on channel attention and multiscale Mel-spectrogram. Biomed. Signal Process. Control. 2024;91:106073. doi: 10.1016/j.bspc.2024.106073.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2024.106073</ArticleId></ArticleIdList></Reference><Reference><Citation>Kim B.J., Kim B.S., Mun J.H., Lim C., Kim K. An accurate deep learning model for wheezing in children using real world data. Sci. Rep. 2022;12:22465. doi: 10.1038/s41598-022-25953-1.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41598-022-25953-1</ArticleId><ArticleId IdType="pmc">PMC9797543</ArticleId><ArticleId IdType="pubmed">36577766</ArticleId></ArticleIdList></Reference><Reference><Citation>Petmezas G., Cheimariotis G.A., Stefanopoulos L., Rocha B., Paiva R.P., Katsaggelos A.K., Maglaveras N. Automated Lung Sound Classification Using a Hybrid CNN-LSTM Network and Focal Loss Function. Sensors. 2022;22:1232. doi: 10.3390/s22031232.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22031232</ArticleId><ArticleId IdType="pmc">PMC8838187</ArticleId><ArticleId IdType="pubmed">35161977</ArticleId></ArticleIdList></Reference><Reference><Citation>Choi Y., Lee H. Interpretation of lung disease classification with light attention connected module. Biomed. Signal Process. Control. 2023;84:104695. doi: 10.1016/j.bspc.2023.104695.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2023.104695</ArticleId><ArticleId IdType="pmc">PMC9978539</ArticleId><ArticleId IdType="pubmed">36879856</ArticleId></ArticleIdList></Reference><Reference><Citation>Serrurier A., Neuschaefer-Rube C., Röhrig R. Past and Trends in Cough Sound Acquisition, Automatic Detection and Automatic Classification: A Comparative Review. Sensors. 2022;22:2896. doi: 10.3390/s22082896.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22082896</ArticleId><ArticleId IdType="pmc">PMC9027375</ArticleId><ArticleId IdType="pubmed">35458885</ArticleId></ArticleIdList></Reference><Reference><Citation>Xia T., Han J., Mascolo C. Exploring machine learning for audio-based respiratory condition screening: A concise review of databases, methods, and open issues. Exp. Biol. Med. 2022;247:2053–2061. doi: 10.1177/15353702221115428.</Citation><ArticleIdList><ArticleId IdType="doi">10.1177/15353702221115428</ArticleId><ArticleId IdType="pmc">PMC9791302</ArticleId><ArticleId IdType="pubmed">35974706</ArticleId></ArticleIdList></Reference><Reference><Citation>Orlandic L., Teijeiro T., Atienza D. The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms. Sci. Data. 2021;8:156. doi: 10.1038/s41597-021-00937-4.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-021-00937-4</ArticleId><ArticleId IdType="pmc">PMC8222356</ArticleId><ArticleId IdType="pubmed">34162883</ArticleId></ArticleIdList></Reference><Reference><Citation>Sharma N., Krishnan P., Kumar R., Ramoji S., Chetupalli S.R., R. N., Ghosh P.K., Ganapathy S. Coswara—A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis; Proceedings of the Interspeech 2020; Virtual Event, Shanghai, China. 25–29 October 2020; pp. 4811–4815.</Citation><ArticleIdList><ArticleId IdType="doi">10.21437/Interspeech.2020-2768</ArticleId></ArticleIdList></Reference><Reference><Citation>Brown C., Chauhan J., Grammenos A., Han J., Hasthanasombat A., Spathis D., Xia T., Cicuta P., Mascolo C. Exploring Automatic Diagnosis of COVID-19 from Crowdsourced Respiratory Sound Data; Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining; Virtual Event. 6–10 July 2020; pp. 3474–3484.</Citation><ArticleIdList><ArticleId IdType="doi">10.1145/3394486.3412865</ArticleId></ArticleIdList></Reference><Reference><Citation>Fakhry A., Jiang X., Xiao J., Chaudhari G., Han A., Khanzada A. Virufy: A Multi-Branch Deep Learning Network for Automated Detection of COVID-19. arXiv. 20212103.01806</Citation></Reference><Reference><Citation>Meister J.A., Nguyen K.A., Luo Z. Audio feature ranking for sound-based COVID-19 patient detection. arXiv. 20212104.07128</Citation></Reference><Reference><Citation>Pahar M., Klopper M., Warren R., Niesler T. COVID-19 cough classification using machine learning and global smartphone recordings. Comput. Biol. Med. 2021;135:104572. doi: 10.1016/j.compbiomed.2021.104572.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2021.104572</ArticleId><ArticleId IdType="pmc">PMC8213969</ArticleId><ArticleId IdType="pubmed">34182331</ArticleId></ArticleIdList></Reference><Reference><Citation>Topuz E.K., Kaya Y. SUPER-COUGH: A Super Learner-based ensemble machine learning method for detecting disease on cough acoustic signals. Biomed. Signal Process. Control. 2024;93:106165. doi: 10.1016/j.bspc.2024.106165.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bspc.2024.106165</ArticleId></ArticleIdList></Reference><Reference><Citation>Rao S., Narayanaswamy V., Esposito M., Thiagarajan J., Spanias A. Deep Learning with hyper-parameter tuning for COVID-19 Cough Detection; Proceedings of the 2021 12th International Conference on Information, Intelligence, Systems &amp; Applications (IISA); Chania Crete, Greece. 12–14 July 2021; pp. 1–5.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/IISA52424.2021.9555564</ArticleId></ArticleIdList></Reference><Reference><Citation>Simonyan K., Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv. 20151409.1556</Citation></Reference><Reference><Citation>Xia T., Spathis D., Brown C., Chauhan J., Grammenos A., Han J., Hasthanasombat A., Bondareva E., Dang T., Floto A., et al. COVID-19 Sounds: A Large-Scale Audio Dataset for Digital Respiratory Screening; Proceedings of the 35th Conference on Neural Information Processing Systems Datasets and Benchmarks Track; Virtual. 6–14 December 2021; pp. 1–13.</Citation></Reference><Reference><Citation>Wall C., Zhang L., Yu Y., Kumar A., Gao R. A Deep Ensemble Neural Network with Attention Mechanisms for Lung Abnormality Classification Using Audio Inputs. Sensors. 2022;22:5566. doi: 10.3390/s22155566.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s22155566</ArticleId><ArticleId IdType="pmc">PMC9332569</ArticleId><ArticleId IdType="pubmed">35898070</ArticleId></ArticleIdList></Reference><Reference><Citation>Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I. Attention is All you Need. Adv. Neural Inf. Process. Syst. 2017;30:1–11.</Citation></Reference><Reference><Citation>Truong T., Mohammadi S., Lenga M. Machine Learning for Health. PMLR; London, UK: 2021. How Transferable are Self-supervised Features in Medical Image Classification Tasks; pp. 54–74.</Citation></Reference><Reference><Citation>Wanasinghe T., Bandara S., Madusanka S., Meedeniya D., Bandara M., de la Torre Díez I. Lung sound classification with multi-feature integration utilizing lightweight CNN model. IEEE Access. 2024;12:21262–21276. doi: 10.1109/ACCESS.2024.3361943.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2024.3361943</ArticleId></ArticleIdList></Reference><Reference><Citation>Griffin D., Lim J. Signal estimation from modified short-time Fourier transform. IEEE Trans. Acoust. Speech Signal Process. 1984;32:236–243. doi: 10.1109/TASSP.1984.1164317.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TASSP.1984.1164317</ArticleId></ArticleIdList></Reference><Reference><Citation>Baevski A., Zhou H., Mohamed A., Auli M. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. arXiv. 20202006.11477</Citation></Reference><Reference><Citation>Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv. 20212010.11929</Citation></Reference><Reference><Citation>Touvron H., Cord M., Douze M., Massa F., Sablayrolles A., Jégou H. Training data-efficient image transformers &amp; distillation through attention; Proceedings of the International Conference on Machine Learning; Virtual. 18–24 July 2021; pp. 10347–10357.</Citation></Reference><Reference><Citation>Bhattacharya D., Sharma N.K., Dutta D., Chetupalli S.R., Mote P., Ganapathy S., Chandrakiran C., Nori S., Suhail K.K., Gonuguntla S., et al. Coswara: A respiratory sounds and symptoms dataset for remote screening of SARS-CoV-2 infection. Sci. Data. 2023;10:397. doi: 10.1038/s41597-023-02266-0.</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41597-023-02266-0</ArticleId><ArticleId IdType="pmc">PMC10287715</ArticleId><ArticleId IdType="pubmed">37349364</ArticleId></ArticleIdList></Reference><Reference><Citation>Loshchilov I., Hutter F. Decoupled Weight Decay Regularization. arXiv. 20191711.05101</Citation></Reference><Reference><Citation>He K., Zhang X., Ren S., Sun J. Deep Residual Learning for Image Recognition; Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Las Vegas, NV, USA. 26 June–1 July 2016; pp. 770–778.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/CVPR.2016.90</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu J., Shen L., Sun G. Squeeze-and-Excitation Networks; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR); Salt Lake City, UT, USA. 18–23 June 2018; pp. 7132–7141.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle>