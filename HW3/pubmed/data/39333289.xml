<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">39333289</PMID><DateCompleted><Year>2024</Year><Month>09</Month><Day>27</Day></DateCompleted><DateRevised><Year>2024</Year><Month>10</Month><Day>02</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>14</Volume><Issue>1</Issue><PubDate><Year>2024</Year><Month>Sep</Month><Day>27</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>A novel hybrid deep learning IChOA-CNN-LSTM model for modality-enriched and multilingual emotion recognition in social media.</ArticleTitle><Pagination><StartPage>22270</StartPage><MedlinePgn>22270</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">22270</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-024-73452-2</ELocationID><Abstract><AbstractText>In the rapidly evolving field of artificial intelligence, the importance of multimodal sentiment analysis has never been more evident, especially amid the ongoing COVID-19 pandemic. Our research addresses the critical need to understand public sentiment across various dimensions of this crisis by integrating data from multiple modalities, such as text, images, audio, and videos sourced from platforms like Twitter. Conventional methods, which primarily focus on text analysis, often fall short in capturing the nuanced intricacies of emotional states, necessitating a more comprehensive approach. To tackle this challenge, our proposed framework introduces a novel hybrid model, IChOA-CNN-LSTM, which leverages Convolutional Neural Networks (CNNs) for precise image feature extraction, Long Short-Term Memory (LSTM) networks for sequential data analysis, and an Improved Chimp Optimization Algorithm for effective feature fusion. Remarkably, our model achieves an impressive accuracy rate of 97.8%, outperforming existing approaches in the field. Additionally, by integrating the GeoCoV19 dataset, we facilitate a comprehensive analysis that spans linguistic and geographical boundaries, enriching our understanding of global pandemic discourse and providing critical insights for informed decision-making in public health crises. Through this holistic approach and innovative techniques, our research significantly advances multimodal sentiment analysis, offering a robust framework for deciphering the complex interplay of emotions during unprecedented global challenges like the COVID-19 pandemic.</AbstractText><CopyrightInformation>© 2024. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Geethanjali</LastName><ForeName>R</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>Research Scholar, Faculty of Information and Communication Engineering, UCE-BIT Campus, Tiruchirappalli, Anna University, Chennai, Tamilnadu, India. geethanjaliyokesh18@gmail.com.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Valarmathi</LastName><ForeName>A</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Assistant Professor, Department of Computer Applications, UCE-BIT Campus, Tiruchirappalli, Anna University, Chennai, Tamilnadu, India.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2024</Year><Month>09</Month><Day>27</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D061108" MajorTopicYN="Y">Social Media</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D004644" MajorTopicYN="Y">Emotions</DescriptorName><QualifierName UI="Q000502" MajorTopicYN="N">physiology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName><QualifierName UI="Q000453" MajorTopicYN="N">epidemiology</QualifierName><QualifierName UI="Q000523" MajorTopicYN="N">psychology</QualifierName><QualifierName UI="Q000821" MajorTopicYN="N">virology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="Y">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086402" MajorTopicYN="N">SARS-CoV-2</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">Multilingual sentiment analysis</Keyword><Keyword MajorTopicYN="N">Multimodal sentiment analysis</Keyword><Keyword MajorTopicYN="N">Proposed IChOA-CNN-LSTM</Keyword><Keyword MajorTopicYN="N">Twitter</Keyword></KeywordList><CoiStatement>We hereby declare that there are no conflicts of interest and the work reported in this paper was done completely without any conflict that could have affected its content. Both authors have contributed to the research and manuscript preparation in a manner befitting academic and ethical standards. The research was conducted with no commercial or financial relationships which would cause a potential conflict of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2024</Year><Month>6</Month><Day>20</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2024</Year><Month>9</Month><Day>17</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2024</Year><Month>9</Month><Day>28</Day><Hour>9</Hour><Minute>48</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2024</Year><Month>9</Month><Day>28</Day><Hour>9</Hour><Minute>47</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2024</Year><Month>9</Month><Day>27</Day><Hour>23</Hour><Minute>30</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>9</Month><Day>27</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">39333289</ArticleId><ArticleId IdType="pmc">PMC11436932</ArticleId><ArticleId IdType="doi">10.1038/s41598-024-73452-2</ArticleId><ArticleId IdType="pii">10.1038/s41598-024-73452-2</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Schukla, A. Sentiment analysis of document based on annotation, CORR J. (2011).</Citation></Reference><Reference><Citation>Sheikh, H. A. &amp; Jaiswal, J. Implementing sentiment analysis on real-time twitter data. J ETIR, 2020, 7, 9.</Citation></Reference><Reference><Citation>Hassena, R. P. Challenges and applications, Int. J. Appl. Innov. Eng. Manag. (IJAIEM), 3(5). (2014).</Citation></Reference><Reference><Citation>Tshimula, J. M., Chikhaoui, B., &amp; Wang, S. COVID-19 detecting depression signals during stay-at-home period (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC9035733</ArticleId><ArticleId IdType="pubmed">35450479</ArticleId></ArticleIdList></Reference><Reference><Citation>Cabezas, J., Moctezuma, D., Fernández-Isabel, A. &amp; Martin de Diego, I. Detecting emotional evolution on Twitter during the covid-19 pandemic using text analysis. Int. J. Environ. Res. Public Health18(13), 6981 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8297321</ArticleId><ArticleId IdType="pubmed">34209977</ArticleId></ArticleIdList></Reference><Reference><Citation>Fountoulakis, K. et al. Self-reported changes in anxiety, depression, and suicidality during the COVID-19 lockdown in Greece. J. Affect. Disord.279, 624–629 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7605790</ArticleId><ArticleId IdType="pubmed">33190113</ArticleId></ArticleIdList></Reference><Reference><Citation>Zammit, O., Smith, S., Windridge, D., &amp; De Raffaele, C. Exposing students to new terminologies while collecting browsing search data (best technical paper). In International Conference on Innovative Techniques and Applications of Artificial Intelligence, pp 3–17. Springer (2020).</Citation></Reference><Reference><Citation>Yin, H., Yang, S., &amp; Li, J. Detecting topic and sentiment dynamics due to COVID-19 pandemic using social media, International Conference on Advanced Data Mining and Applications, Springer, Lecture Notes in Computer Science, vol. 12447 Cham, pp 610–623 (2020).</Citation></Reference><Reference><Citation>Viviani, M. et al. Assessing vulnerability to psychological distress during the COVID-19 pandemic through the analysis of microblogging content. Futur. Gener. Comput. Syst.125, 446–459 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8678930</ArticleId><ArticleId IdType="pubmed">34934256</ArticleId></ArticleIdList></Reference><Reference><Citation>Ghosh, S., &amp; Anwar, T. Depression intensity estimation via social media: A deep learning approach. IEEE Trans. Comput. Soc. Syst.8(6), 1465–1474 (2021).</Citation></Reference><Reference><Citation>Pasumpon, P.A. Performance evaluation and comparison using deep learning techniques in sentiment analysis. J. Soft Comput. Paradigm3(2), 123–134 (2021).</Citation></Reference><Reference><Citation>Tripathi, M. Sentiment analysis of Nepali COVID19 Tweets using NB, SVM AND LSTM. J. Artif. Intell.3(03), 151–168 (2021).</Citation></Reference><Reference><Citation>Medhat, W., Hassan, A., &amp; Korashy, H. Sentiment analysis algorithms and applications, 19 April (2014).</Citation></Reference><Reference><Citation>Gonçalves, P., Araújo, M., Benevenuto, F., Cha, M. Comparing and combining sentiment analysis methods (2013).</Citation></Reference><Reference><Citation>Pereira, M.H.R., Pádua, F.L.C., Pereira, A.C.M., Benevenuto, F., &amp; Dalip, D.H. Fusing audio, textual and visual features for sentiment analysis of news videos. Available at: 10.48550/arXiv.1604.0261 (2016)</Citation></Reference><Reference><Citation>Liu, M., Wang, R., Huang, Z., Shan, S., &amp; Chen, X. Partial least squares regression on grassmannian manifold for emotion recognition. In: Proceedings of the 15th ACM on international conference on multimodal interaction. ACM, pp 525–530 (2013).</Citation></Reference><Reference><Citation>Zafeiriou, S., Zhang, C. &amp; Zhang, Z. A survey on face detection in the wild: Past, present and future. Comput. Vis. Image Underst.138, 1–24 (2015).</Citation></Reference><Reference><Citation>Peng, Y., Ganesh, A., Wright, J., Xu, W. &amp; Ma, Y. RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images. Pattern Anal. Mach. Intell. IEEE Trans.34(11), 2233–2246 (2012).</Citation><ArticleIdList><ArticleId IdType="pubmed">22213763</ArticleId></ArticleIdList></Reference><Reference><Citation>Hassner, T., Harel, S., Paz, E., &amp; Enbar, R. Effective face frontalization in unconstrained images. (2014).</Citation></Reference><Reference><Citation>Geethanjali, R., &amp; Valarmathi, A. “Issues and Future Challenges of Sentiment Analysis for Social Networks- A Survey,” 2022 International Conference on Automation, Computing and Renewable Systems (ICACRS), Pudukkottai, India, 2022, pp 332–339, 10.1109/ICACRS55517.2022.10029070.</Citation></Reference><Reference><Citation>Kahou, S.E., Pal, C., Bouthillier, X., Froumenty, P., Gülçehre, Ç., Memisevic, R., &amp; Wu, Z. Combining modality specific deep neural networks for emotion recognition in video. In: Proceedings of the 15th ACM on international conference on multimodal interaction. ACM, pp 543–550 (2013).</Citation></Reference><Reference><Citation>Liu, M., Wang, R., Li, S., Shan, S., Huang, Z., &amp; Chen, X. Combining multiple kernel methods on iemannian manifold for emotion recognition in the wild. In: Proceedings of the 16th international conference on multimodal interaction. ACM, pp 494–501 (2014).</Citation></Reference><Reference><Citation>Chen, J., Chen, Z., Chi, Z., &amp; Fu, H. Emotion recognition in the wild with feature fusion and multiple kernel learning. In: Proceedings of the 16th international conference on multimodal interaction. ACM, pp 508–513 (2014).</Citation></Reference><Reference><Citation>El Ayadi, M., Kamel, M. S. &amp; Karray, F. Survey on speech emotion recognition: Features, classification schemes, and databases. Pattern Recognit.44(3), 572–587 (2011).</Citation></Reference><Reference><Citation>De la Torre, F. &amp; Cohn, J. F. Facial expression analysis. In Visual analysis ofhumans (eds Moeslund, T. B. et al.) 377–409 (Springer, London, 2011).</Citation></Reference><Reference><Citation>Huang, X., He, Q., Hong, X., Zhao, G., &amp; Pietikainen, M. Improved spatiotemporal local monogenic binary pattern for emotion recognition in the wild. In: Proceedings of the 16th international conference on multimodal interaction. ACM, pp 514–520 (2014).</Citation></Reference><Reference><Citation>Xia, H. &amp; Hoi, S. C. Mkboost: A framework of multiple kernelsboosting. IEEE Trans. Knowl. Data Eng. 25(7), 1574–1586 (2013).</Citation></Reference><Reference><Citation>Bucak, S. S., Jin, R. &amp; Jain, A. K. Multiple kernel learning for visual object recognition: A review. Pattern Anal. Mach. Intell. IEEE Trans.36(7), 1354–1369 (2014).</Citation><ArticleIdList><ArticleId IdType="pubmed">26353308</ArticleId></ArticleIdList></Reference><Reference><Citation>Valstar, M., Girard, J., Almaev, T., McKeown, G., Mehu, M., Yin, L., &amp; Cohn, J. Fera 2015-second facial expression recognition and analysis challenge. Proceeding of the IEEE ICFG (2015).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC5876027</ArticleId><ArticleId IdType="pubmed">29606917</ArticleId></ArticleIdList></Reference><Reference><Citation>Almaev, T.R., &amp; Valstar, M.F. (2013). Local gabor binary patterns from three orthogonal planes for automatic facial expression recognition. In: Affective computing and intelligent interaction (ACII), humaineassociation conference on IEEE, pp 356–361.</Citation></Reference><Reference><Citation>Yue, T., Mao, R., Wang, H., Hu, Z. &amp; Cambria, E. KnowleNet: Knowledge fusion network for multimodal sarcasm detection. Inf. Fusion100, 101921 (2023).</Citation></Reference><Reference><Citation>Frenda, S., Cignarella, A., Basile, V., Bosco, C., Patti, V., &amp; Rosso, P. (2022) The unbearable hurtfulness of sarcasm, Expert Syst. Appl. , 193, 116398</Citation></Reference><Reference><Citation>Gandhi, A., Adhvaryu, K., Poria, S., Cambria, E. &amp; Hussain, A. Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions. Inf. Fusion91, 424–444 (2023).</Citation></Reference><Reference><Citation>Qazi, U., Imran, M. &amp; Ofli, F. GeoCoV19: A dataset of hundreds of millions of multilingual COVID-19 Tweets with location information. SIGSPATIAL Spec.12(1), 6–15. 10.1145/3404820.3404823 (2020).</Citation></Reference><Reference><Citation>https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification</Citation></Reference><Reference><Citation>https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database</Citation></Reference><Reference><Citation>https://www.kaggle.com/datasets/andrewmvd/covid19-cough-audio-classification</Citation></Reference><Reference><Citation>https://twitter.com/COVIDNewsByMIB</Citation></Reference><Reference><Citation>Zhou, X., Feng, J. &amp; Li, Y. Non-intrusive load decomposition based on CNN-LSTM hybrid deep learning model. Energy Rep.7, 1234–1245. 10.1016/j.egyr.2021.09.001 (2021).</Citation></Reference><Reference><Citation>Li, X., Du, Y., &amp; Wang, Y. Time-Enhanced Neighbor-Aware network on irregular time series for sentiment prediction in social networks. Inf. Process. Manag.60(6), 103500 (2023).</Citation></Reference><Reference><Citation>Sikka, K., Dykstra, K., Sathyanarayana, S., Littlewort, G., &amp; Bartlett, M. Multiple kernel learning for emotion recognition in the wild. In: Proceedings of the 15th ACM on international conference on multimodal interaction. ACM, pp 517–524 (2013)</Citation></Reference><Reference><Citation>Amiriparian, S., Christ, L., Kathan, A., Gerczuk, M., Müller, N., Klug, S., Stappen, L., König, A., Cambria, E., Schuller, B., &amp; Eulitz, S. “The MuSe 2024 Multimodal Sentiment Analysis Challenge: Social Perception and Humor Recognition”. Available at: 10.48550/arXiv.2406.07753 (2024)</Citation></Reference><Reference><Citation>Amiriparian, S., Christ, L., König, A., Messner, E.-M., Cowen, A., Cambria, E., &amp; Schuller, B.W. MuSe 2023 Challenge: Multimodal Prediction of Mimicked Emotions, Cross-Cultural Humour, and Personalised Recognition of Affects. In Proceedings of the 31st ACM International Conference on Multimedia (MM’23), October 29-November 2, 2023, Ottawa, Canada. Association for Computing Machinery, Ottawa, Canada. (2023).</Citation></Reference><Reference><Citation>Christ, L., Amiriparian, S., Baird, A., Kathan, A., Müller, N., Klug, S., Gagne, C., Tzirakis, P., Stappen, L., Meßner, E.-M., et al. The muse 2023 multimodal sentiment analysis challenge: Mimicked emotions, cross-cultural humour, and personalisation. In Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions,Humour and Personalisation. 1–10 (2023).</Citation></Reference></ReferenceList></PubmedData></PubmedArticle>