<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">39390053</PMID><DateCompleted><Year>2024</Year><Month>10</Month><Day>10</Day></DateCompleted><DateRevised><Year>2024</Year><Month>10</Month><Day>13</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>14</Volume><Issue>1</Issue><PubDate><Year>2024</Year><Month>Oct</Month><Day>10</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Multi-scale input layers and dense decoder aggregation network for COVID-19 lesion segmentation from CT scans.</ArticleTitle><Pagination><StartPage>23729</StartPage><MedlinePgn>23729</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">23729</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-024-74701-0</ELocationID><Abstract><AbstractText>Accurate segmentation of COVID-19 lesions from medical images is essential for achieving precise diagnosis and developing effective treatment strategies. Unfortunately, this task presents significant challenges, owing to the complex and diverse characteristics of opaque areas, subtle differences between infected and healthy tissue, and the presence of noise in CT images. To address these difficulties, this paper designs a new deep-learning architecture (named MD-Net) based on multi-scale input layers and dense decoder aggregation network for COVID-19 lesion segmentation. In our framework, the U-shaped structure serves as the cornerstone to facilitate complex hierarchical representations essential for accurate segmentation. Then, by introducing the multi-scale input layers (MIL), the network can effectively analyze both fine-grained details and contextual information in the original image. Furthermore, we introduce an SE-Conv module in the encoder network, which can enhance the ability to identify relevant information while simultaneously suppressing the transmission of extraneous or non-lesion information. Additionally, we design a dense decoder aggregation (DDA) module to integrate feature distributions and important COVID-19 lesion information from adjacent encoder layers. Finally, we conducted a comprehensive quantitative analysis and comparison between two publicly available datasets, namely Vid-QU-EX and QaTa-COV19-v2, to assess the robustness and versatility of MD-Net in segmenting COVID-19 lesions. The experimental results show that the proposed MD-Net has superior performance compared to its competitors, and it exhibits higher scores on the Dice value, Matthews correlation coefficient (Mcc), and Jaccard index. In addition, we also conducted ablation studies on the Vid-QU-EX dataset to evaluate the contributions of each key component within the proposed architecture.</AbstractText><CopyrightInformation>© 2024. The Author(s).</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Lan</LastName><ForeName>Xiaoke</ForeName><Initials>X</Initials><AffiliationInfo><Affiliation>College of Internet of Things Technology, Hangzhou Polytechnic, Hangzhou, 311402, China. lxk@mail.hzpt.edu.cn.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Jin</LastName><ForeName>Wenbing</ForeName><Initials>W</Initials><AffiliationInfo><Affiliation>College of Internet of Things Technology, Hangzhou Polytechnic, Hangzhou, 311402, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2024</Year><Month>10</Month><Day>10</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName><QualifierName UI="Q000821" MajorTopicYN="N">virology</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="Y">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086402" MajorTopicYN="Y">SARS-CoV-2</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D007091" MajorTopicYN="N">Image Processing, Computer-Assisted</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D016571" MajorTopicYN="N">Neural Networks, Computer</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading></MeshHeadingList><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">COVID-19</Keyword><Keyword MajorTopicYN="N">Dense decoder aggregation</Keyword><Keyword MajorTopicYN="N">Multi-scale input layers</Keyword><Keyword MajorTopicYN="N">SE-Conv</Keyword><Keyword MajorTopicYN="N">Segmentation</Keyword><Keyword MajorTopicYN="N">U-Net</Keyword></KeywordList><CoiStatement>The authors declare no competing interests.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2024</Year><Month>8</Month><Day>2</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2024</Year><Month>9</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2024</Year><Month>10</Month><Day>11</Day><Hour>0</Hour><Minute>34</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2024</Year><Month>10</Month><Day>11</Day><Hour>0</Hour><Minute>33</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2024</Year><Month>10</Month><Day>10</Day><Hour>23</Hour><Minute>25</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>10</Month><Day>10</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">39390053</ArticleId><ArticleId IdType="pmc">PMC11467340</ArticleId><ArticleId IdType="doi">10.1038/s41598-024-74701-0</ArticleId><ArticleId IdType="pii">10.1038/s41598-024-74701-0</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Long, J., Shelhamer, E. &amp; Darrell, T. Fully convolutional networks for semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 39, 640–651 (2015).</Citation><ArticleIdList><ArticleId IdType="pubmed">27244717</ArticleId></ArticleIdList></Reference><Reference><Citation>Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention 234–241 (2015).</Citation></Reference><Reference><Citation>You, C., Dai, W., Min, Y., Staib, L. &amp; Duncan, J. S. Bootstrapping semi-supervised medical image segmentation with anatomical-aware contrastive distillation. In International Conference on Information Processing in Medical Imaging 641–653 (Springer, 2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10322187</ArticleId><ArticleId IdType="pubmed">37409056</ArticleId></ArticleIdList></Reference><Reference><Citation>You, C. et al. Mine your own anatomy: Revisiting medical image segmentation with extremely limited labels. In IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">39269798</ArticleId></ArticleIdList></Reference><Reference><Citation>You, C. et al. Rethinking semi-supervised medical image segmentation: A variance-reduction perspective. Adv. Neural Inf. Process. Syst. 36, 1 (2024).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11136570</ArticleId><ArticleId IdType="pubmed">38813114</ArticleId></ArticleIdList></Reference><Reference><Citation>You, C. et al. Action++: Improving semi-supervised medical image segmentation with adaptive anatomical contrast. In International Conference on Medical Image Computing and Computer-Assisted Intervention 194–205 (Springer, 2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11136572</ArticleId><ArticleId IdType="pubmed">38813456</ArticleId></ArticleIdList></Reference><Reference><Citation>You, C., Yang, J., Chapiro, J. &amp; Duncan, J. S. Unsupervised wasserstein distance guided domain adaptation for 3d multi-domain liver segmentation. In Interpretable and Annotation-Efficient Learning for Medical Image Computing: Third International Workshop, iMIMIC 2020, Second International Workshop, MIL3ID 2020, and 5th International Workshop, LABELS 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings 3 155–163 (Springer, 2020).</Citation></Reference><Reference><Citation>You, C. et al. Class-aware adversarial transformers for medical image segmentation. Adv. Neural. Inf. Process. Syst. 35, 29582–29596 (2022).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10395073</ArticleId><ArticleId IdType="pubmed">37533756</ArticleId></ArticleIdList></Reference><Reference><Citation>Jin, Q. et al. Inter-and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation. Expert Syst. Appl. 238, 122093 (2024).</Citation></Reference><Reference><Citation>You, C., Zhao, R., Staib, L. H. &amp; Duncan, J. S. Momentum contrastive voxel-wise representation learning for semi-supervised volumetric medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 639–652 (Springer, 2022).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10352821</ArticleId><ArticleId IdType="pubmed">37465615</ArticleId></ArticleIdList></Reference><Reference><Citation>You, C., Zhou, Y., Zhao, R., Staib, L. &amp; Duncan, J. S. Simcvd: Simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation. IEEE Trans. Med. Imaging 41, 2228–2237 (2022).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10325835</ArticleId><ArticleId IdType="pubmed">35320095</ArticleId></ArticleIdList></Reference><Reference><Citation>You, C., Dai, W., Min, Y., Staib, L. &amp; Duncan, J. S. Implicit anatomical rendering for medical image segmentation with stochastic experts. In International Conference on Medical Image Computing and Computer-Assisted Intervention 561–571 (Springer, 2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC11151725</ArticleId><ArticleId IdType="pubmed">38840671</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen, Y. et al. Joint margin adaption and multiscale feature fusion for covid-19 ct images segmentation. Biomed. Signal Process. Control 91, 105912 (2024).</Citation></Reference><Reference><Citation>Li, Y. et al. Cdrime-mtis: An enhanced rime optimization-driven multi-threshold segmentation for covid-19 X-ray images. Comput. Biol. Med. 169, 107838 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">38171259</ArticleId></ArticleIdList></Reference><Reference><Citation>Li, D., Fu, Z. &amp; Xu, J. Stacked-autoencoder-based model for covid-19 diagnosis on ct images. Appl. Intell. 51, 2805–2817 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7652058</ArticleId><ArticleId IdType="pubmed">34764564</ArticleId></ArticleIdList></Reference><Reference><Citation>Ding, X. et al. A novel approach to the technique of lung region segmentation based on a deep learning model to diagnose covid-19 x-ray images. Curr. Med. Imaging 20, 1–11 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">38389381</ArticleId></ArticleIdList></Reference><Reference><Citation>Alsaaidah, B., Mustafa, Z., Al-Hadidi, M. &amp; Alharbi, L. A. Automated identification and categorization of covid-19 via X-ray imagery leveraging roi segmentation and cart model. Traitement Signal 40, 2259–2265 (2023).</Citation></Reference><Reference><Citation>Chen, Y. et al. Bgsnet: A cascaded framework of boundary guided semantic for covid-19 infection segmentation. Biomed. Signal Process. Control 90, 105824 (2024).</Citation></Reference><Reference><Citation>Zhou, T., Lian, B., Wu, C., Chen, H. &amp; Chen, M. U-former: Covid-19 lung infection segmentation based on convolutional neural network and transformer. J. Electron. Imaging 33, 013041 (2024).</Citation></Reference><Reference><Citation>Zhao, S. et al. Scoat-net: A novel network for segmenting covid-19 lung opacification from ct images. Pattern Recogn. 119, 108109 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8189738</ArticleId><ArticleId IdType="pubmed">34127870</ArticleId></ArticleIdList></Reference><Reference><Citation>Devi, M., Singh, S. &amp; Tiwari, S. Covlis–Munet segmentation model for covid-19 lung infection regions in ct images. Neural Comput. Appl. 36, 7265–7278 (2024).</Citation></Reference><Reference><Citation>Liu, S., Cai, T., Tang, X. &amp; Wang, C. Mrl-net: Multi-scale representation learning network for covid-19 lung ct image segmentation. IEEE J. Biomed. Health Inform. 27, 4317–4328 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">37314916</ArticleId></ArticleIdList></Reference><Reference><Citation>Saha, S., Dutta, S., Goswami, B. &amp; Nandi, D. Adu-net: An attention dense u-net based deep supervised dnn for automated lesion segmentation of covid-19 from chest ct images. Biomed. Signal Process. Control 85, 104974 (2023).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC10121143</ArticleId><ArticleId IdType="pubmed">37122956</ArticleId></ArticleIdList></Reference><Reference><Citation>Fan, D.-P. et al. Inf-net: Automatic covid-19 lung infection segmentation from ct images. IEEE Trans. Med. Imaging 39, 2626–2637 (2020).</Citation><ArticleIdList><ArticleId IdType="pubmed">32730213</ArticleId></ArticleIdList></Reference><Reference><Citation>Li, X., Song, J., Jiao, W. &amp; Zheng, Y. Minet: Multi-scale input network for fundus microvascular segmentation. Comput. Biol. Med. 154, 106608 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">36731364</ArticleId></ArticleIdList></Reference><Reference><Citation>Yin, P., Cai, H. &amp; Wu, Q. Df-net: Deep fusion network for multi-source vessel segmentation. Inf. Fusion 78, 199–208 (2022).</Citation></Reference><Reference><Citation>Szegedy, C. et al. Going deeper with convolutions. In Proceedings of the IEEE Conference o Computer Vision and Pattern Recognition 1–9 (2015).</Citation></Reference><Reference><Citation>Tang, H. et al. Htc-net: A hybrid cnn-transformer framework for medical image segmentation. Biomed. Signal Process. Control 88, 105605 (2024).</Citation></Reference><Reference><Citation>Wu, R. et al. Mhorunet: High-order spatial interaction unet for skin lesion segmentation. Biomed. Signal Process. Control 88, 105517 (2024).</Citation></Reference><Reference><Citation>Tahir, A. M. et al. Covid-19 infection localization and severity grading from chest X-ray images. Comput. Biol. Med. 139, 105002 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8556687</ArticleId><ArticleId IdType="pubmed">34749094</ArticleId></ArticleIdList></Reference><Reference><Citation>Yamac, M. et al. Convolutional sparse support estimator-based covid-19 recognition from X-ray images. IEEE Trans. Neural Netw. Learn. Syst. 32, 1810–1820 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC8544941</ArticleId><ArticleId IdType="pubmed">33872157</ArticleId></ArticleIdList></Reference><Reference><Citation>Selvaraj, A. &amp; Nithiyaraj, E. Cedrnn: A convolutional encoder-decoder residual neural network for liver tumour segmentation. Neural Process. Lett. 55, 1605–1624 (2023).</Citation></Reference><Reference><Citation>Trinh, M.-N. et al. An efficientnet-encoder u-net joint residual refinement module with Tversky–Kahneman Baroni–Urbani–Buser loss for biomedical image segmentation. Biomed. Signal Process. Control 83, 104631 (2023).</Citation></Reference><Reference><Citation>Li, J. et al. Class-aware attention network for infectious keratitis diagnosis using corneal photographs. Comput. Biol. Med. 151, 106301 (2022).</Citation><ArticleIdList><ArticleId IdType="pubmed">36403354</ArticleId></ArticleIdList></Reference><Reference><Citation>Oulefki, A., Agaian, S., Trongtirakul, T. &amp; Laouar, A. K. Automatic covid-19 lung infected region segmentation and measurement using ct-scans images. Pattern Recogn. 114, 107747 (2021).</Citation><ArticleIdList><ArticleId IdType="pmc">PMC7605758</ArticleId><ArticleId IdType="pubmed">33162612</ArticleId></ArticleIdList></Reference><Reference><Citation>Hu, K. et al. Dsc-net: A novel interactive two-stream network by combining transformer and cnn for ultrasound image segmentation. IEEE Trans. Instrum. Meas. 72, 5030012 (2023).</Citation></Reference><Reference><Citation>Yu, Z., Yu, L., Zheng, W. &amp; Wang, S. Eiu-net: Enhanced feature extraction and improved skip connections in u-net for skin lesion segmentation. Comput. Biol. Med. 162, 107081 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">37301097</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhao, P., Wang, W., Zhang, G. &amp; Lu, Y. Alleviating pseudo-touching in attention u-net-based binarization approach for the historical Tibetan document images. Neural Comput. Appl. 35, 13791–13802 (2023).</Citation></Reference><Reference><Citation>Muhammad, Z.-U.-D., Huang, Z., Gu, N. &amp; Muhammad, U. Dcanet: Deep context attention network for automatic polyp segmentation. Vis. Comput. 39, 5513–5525 (2023).</Citation></Reference><Reference><Citation>Fu, H. et al. Joint optic disc and cup segmentation based on multi-label deep network and polar transformation. IEEE Trans. Med. Imaging 37, 1597–1605 (2018).</Citation><ArticleIdList><ArticleId IdType="pubmed">29969410</ArticleId></ArticleIdList></Reference><Reference><Citation>Xu, Q., Ma, Z., Na, H. &amp; Duan, W. Dcsau-net: A deeper and more compact split-attention u-net for medical image segmentation. Comput. Biol. Med. 154, 106626 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">36736096</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou, W. et al. Dual-path multi-scale context dense aggregation network for retinal vessel segmentation. Comput. Biol. Med. 164, 107269 (2023).</Citation><ArticleIdList><ArticleId IdType="pubmed">37562323</ArticleId></ArticleIdList></Reference><Reference><Citation>Wu, H., Zhao, Z. &amp; Wang, Z. Meta-unet: Multi-scale efficient transformer attention unet for fast and high-accuracy polyp segmentation. In IEEE Transactions on Automation Science and Engineering (2023).</Citation></Reference><Reference><Citation>Wu, C. et al. Msraformer: Multiscale spatial reverse attention network for polyp segmentation. Comput. Biol. Med. 151, 106274 (2022).</Citation><ArticleIdList><ArticleId IdType="pubmed">36375412</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu, Z. et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 12009–12019 (2022).</Citation></Reference><Reference><Citation>Li, G. et al. Mcafnet: Multiscale cross-layer attention fusion network for honeycomb lung lesion segmentation. Med. Biol. Eng. Comput. 62, 1121–1137 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">38150110</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu, Y., Yao, S., Wang, X., Chen, J. &amp; Li, X. Md-unet: A medical image segmentation network based on mixed depthwise convolution. Med. Biol. Eng. Comput. 62, 1201–1212 (2024).</Citation><ArticleIdList><ArticleId IdType="pubmed">38158549</ArticleId></ArticleIdList></Reference><Reference><Citation>Doc, Y. Z. &amp; Doc, S. W. Duala-net: A generalizable and adaptive network with dual-branch encoder for medical image segmentation. Comput. Methods Progr. Biomed. 243, 107877 (2024).</Citation></Reference></ReferenceList></PubmedData></PubmedArticle>