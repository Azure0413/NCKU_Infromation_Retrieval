<PubmedArticle><MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">39383147</PMID><DateCompleted><Year>2024</Year><Month>10</Month><Day>09</Day></DateCompleted><DateRevised><Year>2024</Year><Month>10</Month><Day>11</Day></DateRevised><Article PubModel="Electronic-eCollection"><Journal><ISSN IssnType="Electronic">1932-6203</ISSN><JournalIssue CitedMedium="Internet"><Volume>19</Volume><Issue>10</Issue><PubDate><Year>2024</Year></PubDate></JournalIssue><Title>PloS one</Title><ISOAbbreviation>PLoS One</ISOAbbreviation></Journal><ArticleTitle>Evaluating Explainable Artificial Intelligence (XAI) techniques in chest radiology imaging through a human-centered Lens.</ArticleTitle><Pagination><StartPage>e0308758</StartPage><MedlinePgn>e0308758</MedlinePgn></Pagination><ELocationID EIdType="pii" ValidYN="Y">e0308758</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.1371/journal.pone.0308758</ELocationID><Abstract><AbstractText>The field of radiology imaging has experienced a remarkable increase in using of deep learning (DL) algorithms to support diagnostic and treatment decisions. This rise has led to the development of Explainable AI (XAI) system to improve the transparency and trust of complex DL methods. However, XAI systems face challenges in gaining acceptance within the healthcare sector, mainly due to technical hurdles in utilizing these systems in practice and the lack of human-centered evaluation/validation. In this study, we focus on visual XAI systems applied to DL-enabled diagnostic system in chest radiography. In particular, we conduct a user study to evaluate two prominent visual XAI techniques from the human perspective. To this end, we created two clinical scenarios for diagnosing pneumonia and COVID-19 using DL techniques applied to chest X-ray and CT scans. The achieved accuracy rates were 90% for pneumonia and 98% for COVID-19. Subsequently, we employed two well-known XAI methods, Grad-CAM (Gradient-weighted Class Activation Mapping) and LIME (Local Interpretable Model-agnostic Explanations), to generate visual explanations elucidating the AI decision-making process. The visual explainability results were shared through a user study, undergoing evaluation by medical professionals in terms of clinical relevance, coherency, and user trust. In general, participants expressed a positive perception of the use of XAI systems in chest radiography. However, there was a noticeable lack of awareness regarding their value and practical aspects. Regarding preferences, Grad-CAM showed superior performance over LIME in terms of coherency and trust, although concerns were raised about its clinical usability. Our findings highlight key user-driven explainability requirements, emphasizing the importance of multi-modal explainability and the necessity to increase awareness of XAI systems among medical practitioners. Inclusive design was also identified as a crucial need to ensure better alignment of these systems with user needs.</AbstractText><CopyrightInformation>Copyright: © 2024 E. Ihongbe et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>E Ihongbe</LastName><ForeName>Izegbua</ForeName><Initials>I</Initials><Identifier Source="ORCID">0009-0003-3630-5404</Identifier><AffiliationInfo><Affiliation>School of Computer Science and Digital Technologies, Aston University, Birmingham, United Kingdom.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Fouad</LastName><ForeName>Shereen</ForeName><Initials>S</Initials><Identifier Source="ORCID">0000-0002-4965-7017</Identifier><AffiliationInfo><Affiliation>School of Computer Science and Digital Technologies, Aston University, Birmingham, United Kingdom.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>F Mahmoud</LastName><ForeName>Taha</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>Medical Imaging Department, University Hospital of Sharjah, Sharjah, United Arab Emirates.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Rajasekaran</LastName><ForeName>Arvind</ForeName><Initials>A</Initials><AffiliationInfo><Affiliation>Sandwell And West Birmingham Hospitals NHS Trust, Birmingham, United Kingdom.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Bhatia</LastName><ForeName>Bahadar</ForeName><Initials>B</Initials><AffiliationInfo><Affiliation>Sandwell And West Birmingham Hospitals NHS Trust, Birmingham, United Kingdom.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>University of Leicester, Leicester, United Kingdom.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2024</Year><Month>10</Month><Day>09</Day></ArticleDate></Article><MedlineJournalInfo><Country>United States</Country><MedlineTA>PLoS One</MedlineTA><NlmUniqueID>101285081</NlmUniqueID><ISSNLinking>1932-6203</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset><MeshHeadingList><MeshHeading><DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086382" MajorTopicYN="Y">COVID-19</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000077321" MajorTopicYN="Y">Deep Learning</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D013902" MajorTopicYN="Y">Radiography, Thoracic</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D001185" MajorTopicYN="Y">Artificial Intelligence</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D000086402" MajorTopicYN="Y">SARS-CoV-2</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D014057" MajorTopicYN="N">Tomography, X-Ray Computed</DescriptorName><QualifierName UI="Q000379" MajorTopicYN="N">methods</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D011014" MajorTopicYN="N">Pneumonia</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000465" MajorTopicYN="N">Algorithms</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D008297" MajorTopicYN="N">Male</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D005260" MajorTopicYN="N">Female</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D058873" MajorTopicYN="N">Pandemics</DescriptorName></MeshHeading><MeshHeading><DescriptorName UI="D018352" MajorTopicYN="N">Coronavirus Infections</DescriptorName><QualifierName UI="Q000000981" MajorTopicYN="N">diagnostic imaging</QualifierName></MeshHeading><MeshHeading><DescriptorName UI="D000073640" MajorTopicYN="N">Betacoronavirus</DescriptorName></MeshHeading></MeshHeadingList><CoiStatement>The authors have declared that no competing interests exist.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2024</Year><Month>1</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2024</Year><Month>7</Month><Day>30</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2024</Year><Month>10</Month><Day>9</Day><Hour>22</Hour><Minute>18</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2024</Year><Month>10</Month><Day>9</Day><Hour>22</Hour><Minute>17</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2024</Year><Month>10</Month><Day>9</Day><Hour>13</Hour><Minute>33</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>10</Month><Day>9</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">39383147</ArticleId><ArticleId IdType="pmc">PMC11463756</ArticleId><ArticleId IdType="doi">10.1371/journal.pone.0308758</ArticleId><ArticleId IdType="pii">PONE-D-24-03792</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>World Health Organization, Respiratory diseases; 2023.</Citation></Reference><Reference><Citation>Song TH, Landini G, Fouad S, Mehanna H. Epithelial segmentation from in situ hybridisation histological samples using a deep central attention learning approach. In: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE; 2019. p. 1527–1531.</Citation></Reference><Reference><Citation>Shen D, Wu G, Suk HI. Deep learning in medical image analysis. Annual review of biomedical engineering. 2017;19:221–248. doi: 10.1146/annurev-bioeng-071516-044442</Citation><ArticleIdList><ArticleId IdType="doi">10.1146/annurev-bioeng-071516-044442</ArticleId><ArticleId IdType="pmc">PMC5479722</ArticleId><ArticleId IdType="pubmed">28301734</ArticleId></ArticleIdList></Reference><Reference><Citation>Fouad S, Landini G, Robinson M, Song TH, Mehanna H. Human papilloma virus detection in oropharyngeal carcinomas with in situ hybridisation using hand crafted morphological features and deep central attention residual networks. Computerized Medical Imaging and Graphics. 2021;88:101853. doi: 10.1016/j.compmedimag.2021.101853</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compmedimag.2021.101853</ArticleId><ArticleId IdType="pubmed">33508566</ArticleId></ArticleIdList></Reference><Reference><Citation>Yadav SS, Jadhav SM. Deep convolutional neural network based medical image classification for disease diagnosis. Journal of Big data. 2019;6(1):1–18. doi: 10.1186/s40537-019-0276-2</Citation><ArticleIdList><ArticleId IdType="doi">10.1186/s40537-019-0276-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Mporas I, Naronglerdrit P. COVID-19 identification from chest X-Rays. In: 2020 International conference on biomedical innovations and applications (BIA). IEEE; 2020. p. 69–72.</Citation></Reference><Reference><Citation>Voigt P, Von dem Bussche A. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed, Cham: Springer International Publishing. 2017;10(3152676):10–5555.</Citation></Reference><Reference><Citation>Van der Velden BH, Kuijf HJ, Gilhuijs KG, Viergever MA. Explainable artificial intelligence (XAI) in deep learning-based medical image analysis. Medical Image Analysis. 2022;79:102470. doi: 10.1016/j.media.2022.102470</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2022.102470</ArticleId><ArticleId IdType="pubmed">35576821</ArticleId></ArticleIdList></Reference><Reference><Citation>Saeed W, Omlin C. Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities. Knowledge-Based Systems. 2023;263:110273. doi: 10.1016/j.knosys.2023.110273</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.knosys.2023.110273</ArticleId></ArticleIdList></Reference><Reference><Citation>Ieracitano C, Mammone N, Spagnolo F, Frustaci F, Perri S, Corsonello P, et al.. An explainable embedded neural system for on-board ship detection from optical satellite imagery. Engineering Applications of Artificial Intelligence. 2024;133:108517. doi: 10.1016/j.engappai.2024.108517</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.engappai.2024.108517</ArticleId></ArticleIdList></Reference><Reference><Citation>Sharma B, Sharma L, Lal C, Roy S. Explainable artificial intelligence for intrusion detection in IoT networks: A deep learning based approach. Expert Systems with Applications. 2024;238:121751. doi: 10.1016/j.eswa.2023.121751</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2023.121751</ArticleId></ArticleIdList></Reference><Reference><Citation>Antoniadi AM, Du Y, Guendouz Y, Wei L, Mazo C, Becker BA, et al.. Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: a systematic review. Applied Sciences. 2021;11(11):5088. doi: 10.3390/app11115088</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/app11115088</ArticleId></ArticleIdList></Reference><Reference><Citation>Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE international conference on computer vision; 2017. p. 618–626.</Citation></Reference><Reference><Citation>Ribeiro MT, Singh S, Guestrin C. “Why should i trust you?” Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016. p. 1135–1144.</Citation></Reference><Reference><Citation>Lundberg SM, Lee SI. A unified approach to interpreting model predictions. Advances in neural information processing systems. 2017;30.</Citation></Reference><Reference><Citation>Barzas K, Fouad S, Jasa G, Landini G. An Explainable Deep Learning Framework for Mandibular Canal Segmentation from Cone Beam Computed Tomography volumes. In: The 12th International Conference on Computational Advances in Bio and Medical Sciences. Springer; 2023.</Citation></Reference><Reference><Citation>Sun J, Chakraborti T, Noble JA. A Comparative Study of Explainer Modules Applied to Automated Skin Lesion Classification. In: XI-ML@ KI; 2020.</Citation></Reference><Reference><Citation>Holzinger A, Saranti A, Molnar C, Biecek P, Samek W. In: Explainable AI Methods—A Brief Overview. Springer International Publishing; 2022.</Citation></Reference><Reference><Citation>Cian D, van Gemert J, Lengyel A. Evaluating the performance of the LIME and Grad-CAM explanation methods on a LEGO multi-label image classification task; 2020.</Citation></Reference><Reference><Citation>van der Waa J, Nieuwburg E, Cremers A, Neerincx M. Evaluating XAI: A comparison of rule-based and example-based explanations. Artificial Intelligence. 2021;291:103404. doi: 10.1016/j.artint.2020.103404</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.artint.2020.103404</ArticleId></ArticleIdList></Reference><Reference><Citation>Schoonderwoerd TA, Jorritsma W, Neerincx MA, Van Den Bosch K. Human-centered XAI: Developing design patterns for explanations of clinical decision support systems. International Journal of Human-Computer Studies. 2021;154:102684. doi: 10.1016/j.ijhcs.2021.102684</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ijhcs.2021.102684</ArticleId></ArticleIdList></Reference><Reference><Citation>Liao QV, Varshney KR. Human-Centered Explainable AI (XAI): From Algorithms to User Experiences. CoRR. 2021;abs/2110.10790.</Citation></Reference><Reference><Citation>Chen H, Gomez C, Huang CM, Unberath M. Explainable medical imaging AI needs human-centered design: guidelines and evidence from a systematic review. NPJ digital medicine. 2022;5(1):156. doi: 10.1038/s41746-022-00699-2</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-022-00699-2</ArticleId><ArticleId IdType="pmc">PMC9581990</ArticleId><ArticleId IdType="pubmed">36261476</ArticleId></ArticleIdList></Reference><Reference><Citation>Jin W, Li X, Fatehi M, Hamarneh G. Guidelines and evaluation of clinical explainable AI in medical image analysis. Medical Image Analysis. 2023;84:102684. doi: 10.1016/j.media.2022.102684</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.media.2022.102684</ArticleId><ArticleId IdType="pubmed">36516555</ArticleId></ArticleIdList></Reference><Reference><Citation>Ihongbe IE, Fouad S. User Studies in Evaluating Explainable Artificial Intelligence (XAI) Techniques in Medical Radiology Imaging. MSc Dissertation in Applied Artificial Intelligence, Aston University, United Kingdom; 2023.</Citation></Reference><Reference><Citation>Rajpurkar P, O’Connell C, Schechter A, Asnani N, Li J, Kiani A, et al.. CheXaid: deep learning assistance for physician diagnosis of tuberculosis using chest x-rays in patients with HIV. NPJ digital medicine. 2020;3(1):115. doi: 10.1038/s41746-020-00322-2</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/s41746-020-00322-2</ArticleId><ArticleId IdType="pmc">PMC7481246</ArticleId><ArticleId IdType="pubmed">32964138</ArticleId></ArticleIdList></Reference><Reference><Citation>Yagin FH, Cicek İB, Alkhateeb A, Yagin B, Colak C, Azzeh M, et al.. Explainable artificial intelligence model for identifying COVID-19 gene biomarkers. Computers in Biology and Medicine. 2023;154:106619. doi: 10.1016/j.compbiomed.2023.106619</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compbiomed.2023.106619</ArticleId><ArticleId IdType="pmc">PMC9889119</ArticleId><ArticleId IdType="pubmed">36738712</ArticleId></ArticleIdList></Reference><Reference><Citation>Vilone G, Longo L. Notions of explainability and evaluation approaches for explainable artificial intelligence. Information Fusion. 2021;76:89–106. doi: 10.1016/j.inffus.2021.05.009</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.inffus.2021.05.009</ArticleId></ArticleIdList></Reference><Reference><Citation>Bibal A, Frénay B. Interpretability of machine learning models and representations: an introduction. In: 24th european symposium on artificial neural networks, computational intelligence and machine learning. CIACO; 2016. p. 77–82.</Citation></Reference><Reference><Citation>Ye Q, Xia J, Yang G. Explainable AI for COVID-19 CT classifiers: an initial comparison study. In: 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS). IEEE; 2021. p. 521–526.</Citation></Reference><Reference><Citation>Bussone A, Stumpf S, O’Sullivan D. The role of explanations on trust and reliance in clinical decision support systems. In: 2015 international conference on healthcare informatics. IEEE; 2015. p. 160–169.</Citation></Reference><Reference><Citation>Tonekaboni S, Joshi S, McCradden MD, Goldenberg A. What clinicians want: contextualizing explainable machine learning for clinical end use. In: Machine learning for healthcare conference. PMLR; 2019. p. 359–380.</Citation></Reference><Reference><Citation>Kermany D, Zhang K, Goldbaum M. Labeled optical coherence tomography (oct) and chest x-ray images for classification. Mendeley data. 2018;2(2):651.</Citation></Reference><Reference><Citation>Act A. Health insurance portability and accountability act of 1996. Public law. 1996;104:191.</Citation><ArticleIdList><ArticleId IdType="pubmed">16477734</ArticleId></ArticleIdList></Reference><Reference><Citation>Soares E, Angelov P, Biaso S, Froes MH, Abe DK. SARS-CoV-2 CT-scan dataset: A large dataset of real patients CT scans for SARS-CoV-2 identification. MedRxiv. 2020; p. 2020–04.</Citation></Reference><Reference><Citation>Kakogeorgiou I, Karantzalos K. Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing. International Journal of Applied Earth Observation and Geoinformation. 2021;103:102520. doi: 10.1016/j.jag.2021.102520</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jag.2021.102520</ArticleId></ArticleIdList></Reference><Reference><Citation>Muhammad MB, Yeasin M. Eigen-cam: Class activation map using principal components. In: 2020 international joint conference on neural networks (IJCNN). IEEE; 2020. p. 1–7.</Citation></Reference><Reference><Citation>Wang X, Peng Y, Lu L, Lu Z, Summers RM. Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2018. p. 9049–9058.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle>