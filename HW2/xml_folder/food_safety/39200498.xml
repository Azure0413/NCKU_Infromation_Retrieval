<PubmedArticle><MedlineCitation Status="PubMed-not-MEDLINE" Owner="NLM"><PMID Version="1">39200498</PMID><DateRevised><Year>2024</Year><Month>09</Month><Day>01</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Print">2304-8158</ISSN><JournalIssue CitedMedium="Print"><Volume>13</Volume><Issue>16</Issue><PubDate><Year>2024</Year><Month>Aug</Month><Day>17</Day></PubDate></JournalIssue><Title>Foods (Basel, Switzerland)</Title><ISOAbbreviation>Foods</ISOAbbreviation></Journal><ArticleTitle>CR-YOLOv9: Improved YOLOv9 Multi-Stage Strawberry Fruit Maturity Detection Application Integrated with CRNET.</ArticleTitle><ELocationID EIdType="pii" ValidYN="Y">2571</ELocationID><ELocationID EIdType="doi" ValidYN="Y">10.3390/foods13162571</ELocationID><Abstract><AbstractText>Strawberries are a commonly used agricultural product in the food industry. In the traditional production model, labor costs are high, and extensive picking techniques can result in food safety issues, like poor taste and fruit rot. In response to the existing challenges of low detection accuracy and slow detection speed in the assessment of strawberry fruit maturity in orchards, a CR-YOLOv9 multi-stage method for strawberry fruit maturity detection was introduced. The composite thinning network, CRNet, is utilized for target fusion, employing multi-branch blocks to enhance images by restoring high-frequency details. To address the issue of low computational efficiency in the multi-head self-attention (MHSA) model due to redundant attention heads, the design concept of CGA is introduced. This concept aligns input feature grouping with the number of attention heads, offering the distinct segmentation of complete features for each attention head, thereby reducing computational redundancy. A hybrid operator, ACmix, is proposed to enhance the efficiency of image classification and target detection. Additionally, the Inner-IoU concept, in conjunction with Shape-IoU, is introduced to replace the original loss function, thereby enhancing the accuracy of detecting small targets in complex scenes. The experimental results demonstrate that CR-YOLOv9 achieves a precision rate of 97.52%, a recall rate of 95.34%, and an mAP@50 of 97.95%. These values are notably higher than those of YOLOv9 by 4.2%, 5.07%, and 3.34%. Furthermore, the detection speed of CR-YOLOv9 is 84, making it suitable for the real-time detection of strawberry ripeness in orchards. The results demonstrate that the CR-YOLOv9 algorithm discussed in this study exhibits high detection accuracy and rapid detection speed. This enables more efficient and automated strawberry picking, meeting the public's requirements for food safety.</AbstractText></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Ye</LastName><ForeName>Rong</ForeName><Initials>R</Initials><AffiliationInfo><Affiliation>College of Food Science and Technology, Yunnan Agricultural University, Kunming 650201, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Shao</LastName><ForeName>Guoqi</ForeName><Initials>G</Initials><Identifier Source="ORCID">0009-0008-8964-7826</Identifier><AffiliationInfo><Affiliation>The Key Laboratory for Crop Production and Smart Agriculture of Yunnan Province, Yunnan Agricultural University, Kunming 650201, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Gao</LastName><ForeName>Quan</ForeName><Initials>Q</Initials><Identifier Source="ORCID">0009-0002-4569-9305</Identifier><AffiliationInfo><Affiliation>College of Big Data, Yunnan Agricultural University, Kunming 650201, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Zhang</LastName><ForeName>Hongrui</ForeName><Initials>H</Initials><Identifier Source="ORCID">0000-0002-0089-1099</Identifier><AffiliationInfo><Affiliation>College of Plant Protection, Yunnan Agricultural University, Kunming 650201, China.</Affiliation></AffiliationInfo></Author><Author ValidYN="Y"><LastName>Li</LastName><ForeName>Tong</ForeName><Initials>T</Initials><AffiliationInfo><Affiliation>The Key Laboratory for Crop Production and Smart Agriculture of Yunnan Province, Yunnan Agricultural University, Kunming 650201, China.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>College of Big Data, Yunnan Agricultural University, Kunming 650201, China.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><GrantList CompleteYN="Y"><Grant><GrantID>202302AE090020</GrantID><Agency>Tong Li</Agency><Country /></Grant></GrantList><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2024</Year><Month>08</Month><Day>17</Day></ArticleDate></Article><MedlineJournalInfo><Country>Switzerland</Country><MedlineTA>Foods</MedlineTA><NlmUniqueID>101670569</NlmUniqueID><ISSNLinking>2304-8158</ISSNLinking></MedlineJournalInfo><KeywordList Owner="NOTNLM"><Keyword MajorTopicYN="N">CGA</Keyword><Keyword MajorTopicYN="N">CRNet</Keyword><Keyword MajorTopicYN="N">Shape-IoU</Keyword><Keyword MajorTopicYN="N">YOLOv9</Keyword><Keyword MajorTopicYN="N">maturity classification</Keyword><Keyword MajorTopicYN="N">target detection</Keyword></KeywordList><CoiStatement>The authors declare no conflicts of interest.</CoiStatement></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="received"><Year>2024</Year><Month>6</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="revised"><Year>2024</Year><Month>8</Month><Day>13</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2024</Year><Month>8</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="medline"><Year>2024</Year><Month>8</Month><Day>31</Day><Hour>9</Hour><Minute>49</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2024</Year><Month>8</Month><Day>31</Day><Hour>9</Hour><Minute>48</Minute></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2024</Year><Month>8</Month><Day>29</Day><Hour>1</Hour><Minute>8</Minute></PubMedPubDate><PubMedPubDate PubStatus="pmc-release"><Year>2024</Year><Month>8</Month><Day>17</Day></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">39200498</ArticleId><ArticleId IdType="pmc">PMC11354223</ArticleId><ArticleId IdType="doi">10.3390/foods13162571</ArticleId><ArticleId IdType="pii">foods13162571</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Giampieri F., Tulipani S., Alvarez-Suarez J.M., Quiles J.L., Mezzetti B., Battino M. The strawberry: Composition, nutritional quality, and impact on human health. Nutrition. 2012;28:9–19. doi: 10.1016/j.nut.2011.08.009.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.nut.2011.08.009</ArticleId><ArticleId IdType="pubmed">22153122</ArticleId></ArticleIdList></Reference><Reference><Citation>Sun J., Chen Y., Zhou X., Shen J., Wu X. Fast and accurate recognition of the strawberries in greenhouse based on improved YOLOv4-Tiny model. Trans. Chin. Soc. Agric. Eng. 2022;38:195–203.</Citation></Reference><Reference><Citation>Renfan C., Zhi X., Cheng L. YOLO-ODM based rapid detection of strawberry ripeness in greenhouse. J. Huazhong Agric. Univ. 2023;42:262–269. doi: 10.13300/j.cnki.hnlkxb.2023.04.030.</Citation><ArticleIdList><ArticleId IdType="doi">10.13300/j.cnki.hnlkxb.2023.04.030</ArticleId></ArticleIdList></Reference><Reference><Citation>Gao Z., Shao Y., Xuan G., Wang Y., Liu Y., Han X. Real-time hyperspectral imaging for the in-field estimation of strawberry ripeness with deep learning. Artif. Intell. Agric. 2020;4:31–38. doi: 10.1016/j.aiia.2020.04.003.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.aiia.2020.04.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou X., Lee W.S., Ampatzidis Y., Chen Y., Peres N., Fraisse C. Strawberry maturity classification from UAV and near-ground imaging using deep learning. Smart Agric. Technol. 2021;1:100001. doi: 10.1016/j.atech.2021.100001.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.atech.2021.100001</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhou L., Zhang C., Liu F., Qiu Z., He Y. Application of deep learning in food: A review. Compr. Rev. Food Sci. Food Saf. 2019;18:1793–1811. doi: 10.1111/1541-4337.12492.</Citation><ArticleIdList><ArticleId IdType="doi">10.1111/1541-4337.12492</ArticleId><ArticleId IdType="pubmed">33336958</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang Y., Deng L., Zhu H., Wang W., Ren Z., Zhou Q., Lu S., Sun S., Zhu Z., Gorriz J.M., et al. Deep learning in food category recognition. Inf. Fusion. 2023;98:101859. doi: 10.1016/j.inffus.2023.101859.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.inffus.2023.101859</ArticleId></ArticleIdList></Reference><Reference><Citation>Mohanty S.P., Singhal G., Scuccimarra E.A., Kebaili D., Héritier H., Boulanger V., Salathé M. The food recognition benchmark: Using deep learning to recognize food in images. Front. Nutr. 2022;9:875143. doi: 10.3389/fnut.2022.875143.</Citation><ArticleIdList><ArticleId IdType="doi">10.3389/fnut.2022.875143</ArticleId><ArticleId IdType="pmc">PMC9121091</ArticleId><ArticleId IdType="pubmed">35600815</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang J., Xia L., Liu H., Zhao C., Ming S., Wu J. Colorimetric microneedle sensor using deep learning algorithm for meat freshness monitoring. Chem. Eng. J. 2024;481:148474. doi: 10.1016/j.cej.2023.148474.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.cej.2023.148474</ArticleId></ArticleIdList></Reference><Reference><Citation>Lin Y., Ma J., Sun D.W., Cheng J.H., Zhou C. Fast real-time monitoring of meat freshness based on fluorescent sensing array and deep learning: From development to deployment. Food Chem. 2024;448:139078. doi: 10.1016/j.foodchem.2024.139078.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.foodchem.2024.139078</ArticleId><ArticleId IdType="pubmed">38527403</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen S., Xiong J., Jiao J., Xie Z., Huo Z., Hu W. Citrus fruits maturity detection in natural environments based on convolutional neural networks and visual saliency map. Precis. Agric. 2022;23:1515–1531. doi: 10.1007/s11119-022-09895-2.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11119-022-09895-2</ArticleId></ArticleIdList></Reference><Reference><Citation>Ashtiani S.H.M., Javanmardi S., Jahanbanifard M., Martynenko A., Verbeek F.J. Detection of mulberry ripeness stages using deep learning models. IEEE Access. 2021;9:100380–100394. doi: 10.1109/ACCESS.2021.3096550.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2021.3096550</ArticleId></ArticleIdList></Reference><Reference><Citation>Faisal M., Albogamy F., Elgibreen H., Algabri M., Alqershi F.A. Deep learning and computer vision for estimating date fruits type, maturity level, and weight. IEEE Access. 2020;8:206770–206782. doi: 10.1109/ACCESS.2020.3037948.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2020.3037948</ArticleId></ArticleIdList></Reference><Reference><Citation>Saranya N., Srinivasan K., Kumar S.K.P. Banana ripeness stage identification: A deep learning approach. J. Ambient Intell. Humaniz. Comput. 2022;13:4033–4039. doi: 10.1007/s12652-021-03267-w.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s12652-021-03267-w</ArticleId></ArticleIdList></Reference><Reference><Citation>Mesa A.R., Chiang J.Y. Multi-input deep learning model with RGB and hyperspectral imaging for banana grading. Agriculture. 2021;11:687. doi: 10.3390/agriculture11080687.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/agriculture11080687</ArticleId></ArticleIdList></Reference><Reference><Citation>Hossain M.S., Al-Hammadi M., Muhammad G. Automatic fruit classification using deep learning for industrial applications. IEEE Trans. Ind. Inform. 2018;15:1027–1034. doi: 10.1109/TII.2018.2875149.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TII.2018.2875149</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang H., Cloutier R.S. Review on one-stage object detection based on deep learning. EAI Endorsed Trans. E-Learn. 2021;7:e5. doi: 10.4108/eai.9-6-2022.174181.</Citation><ArticleIdList><ArticleId IdType="doi">10.4108/eai.9-6-2022.174181</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang L., Bai J., Li W., Jiang J. Research Progress of YOLO Series Target Detection Algorithms. Comput. Eng. Appl. 2023;59:15–29.</Citation></Reference><Reference><Citation>Liu Z., Xiao D. Recognition method of mature strawberry based on improved ssd deep convolution neural network; Bio-inspired Computing: Theories and Applications: 14th International Conference, BIC-TA 2019; Zhengzhou, China. 22–25 November 2019; Singapore: Springer; 2020. pp. 271–281. Revised Selected Papers, Part II 14.</Citation></Reference><Reference><Citation>Du L., Zhang R., Wang X. Overview of two-stage object detection algorithms. J. Phys. Conf. Ser. 2020;1544:012033. doi: 10.1088/1742-6596/1544/1/012033.</Citation><ArticleIdList><ArticleId IdType="doi">10.1088/1742-6596/1544/1/012033</ArticleId></ArticleIdList></Reference><Reference><Citation>Hsieh K.W., Huang B.Y., Hsiao K.Z., Tuan Y.H., Shih F.P., Hsieh L.C., Chen S., Yang I.C. Fruit maturity and location identification of beef tomato using R-CNN and binocular imaging technology. J. Food Meas. Charact. 2021;15:5170–5180. doi: 10.1007/s11694-021-01074-7.</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s11694-021-01074-7</ArticleId></ArticleIdList></Reference><Reference><Citation>Girshick R. Fast r-cnn; Proceedings of the IEEE International Conference on Computer Vision; Santiago, Chile. 7–13 December 2015; pp. 1440–1448.</Citation></Reference><Reference><Citation>Parvathi S., Selvi S.T. Detection of maturity stages of coconuts in complex background using Faster R-CNN model. Biosyst. Eng. 2021;202:119–132. doi: 10.1016/j.biosystemseng.2020.12.002.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.biosystemseng.2020.12.002</ArticleId></ArticleIdList></Reference><Reference><Citation>Wang Y., Yan G., Meng Q., Yao T., Han J., Zhang B. DSE-YOLO: Detail semantics enhancement YOLO for multi-stage strawberry detection. Comput. Electron. Agric. 2022;198:107057. doi: 10.1016/j.compag.2022.107057.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compag.2022.107057</ArticleId></ArticleIdList></Reference><Reference><Citation>An Q., Wang K., Li Z., Song C., Tang X., Song J. Real-time monitoring method of strawberry fruit growth state based on YOLO improved model. IEEE Access. 2022;10:124363–124372. doi: 10.1109/ACCESS.2022.3220234.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/ACCESS.2022.3220234</ArticleId></ArticleIdList></Reference><Reference><Citation>Cuong N.H.H., Trinh T.H., Meesad P., Nguyen T.T. Improved YOLO object detection algorithm to detect ripe pineapple phase. J. Intell. Fuzzy Syst. 2022;43:1365–1381. doi: 10.3233/JIFS-213251.</Citation><ArticleIdList><ArticleId IdType="doi">10.3233/JIFS-213251</ArticleId></ArticleIdList></Reference><Reference><Citation>Chen W., Liu M., Zhao C., Li X., Wang Y. MTD-YOLO: Multi-task deep convolutional neural network for cherry tomato fruit bunch maturity detection. Comput. Electron. Agric. 2024;216:108533. doi: 10.1016/j.compag.2023.108533.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compag.2023.108533</ArticleId></ArticleIdList></Reference><Reference><Citation>Li P., Zheng J., Li P., Long H., Li M., Gao L. Tomato maturity detection and counting model based on MHSA-YOLOv8. Sensors. 2023;23:6701. doi: 10.3390/s23156701.</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/s23156701</ArticleId><ArticleId IdType="pmc">PMC10422388</ArticleId><ArticleId IdType="pubmed">37571485</ArticleId></ArticleIdList></Reference><Reference><Citation>You S., Zhang C.Q., Wang C., Ning S. Research on cherry maturity detection based on improved DS-YOLOV8. arXiv. 2024 doi: 10.21203/rs.3.rs-3937772/v1.</Citation><ArticleIdList><ArticleId IdType="doi">10.21203/rs.3.rs-3937772/v1</ArticleId></ArticleIdList></Reference><Reference><Citation>Yang S., Wang W., Gao S., Deng Z. Strawberry ripeness detection based on YOLOv8 algorithm fused with LW-Swin. Transform. Comput. Electron. Agric. 2023;215:108360. doi: 10.1016/j.compag.2023.108360.</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.compag.2023.108360</ArticleId></ArticleIdList></Reference><Reference><Citation>Abdelrahman A., Mahmoud A., Radu T., Michael S.B. Ntire 2020 challenge on real image denoising: Dataset, methods and results; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops; Seattle, WA, USA. 14–19 June 2020; pp. 496–497.</Citation></Reference><Reference><Citation>Tim B., Ben M., Tianfan X., Jiawen C., Dillon S., Barron J.T. Unprocessing images for learned raw denoising; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition; Long Beach, CA, USA. 16–17 June 2019; pp. 11036–11045.</Citation></Reference><Reference><Citation>Mao X., Liu Y., Liu F., Li Q., Shen W. Intriguing findings of frequency selection for image deblurring; Proceedings of the AAAI Conference on Artificial Intelligence; Washington, DC, USA. 7–14 February 2023; pp. 1905–1913.</Citation></Reference><Reference><Citation>Nah S., Kim T.H., Lee K.M. Deep multi-scale convolutional neural network for dynamic scene deblurring; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; Honolulu, HI, USA. 21–26 July 2017; pp. 3883–3891.</Citation></Reference><Reference><Citation>Zhang Z., Xu R., Liu M., Yan Z., Zuo W. Self-supervised image restoration with blurry and noisy pairs. Adv. Neural Inf. Process. Syst. 2022;35:29179–29191.</Citation></Reference><Reference><Citation>Liu Z., Wang Y., Zeng B., Liu S. Ghost-free high dynamic range imaging with context-aware transformer; Proceedings of the European Conference on Computer Vision; Tel Aviv, Israel. 23–27 October 2022; Berlin/Heidelberg, Germany: Springer; 2022. pp. 344–360.</Citation></Reference><Reference><Citation>Niu Y., Wu J., Liu W., Guo W., Lau R.W. Hdr-gan: Hdr image reconstruction from multi-exposed ldr images with large motions. IEEE Trans. Image Process. 2021;30:3885–3896. doi: 10.1109/TIP.2021.3064433.</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/TIP.2021.3064433</ArticleId><ArticleId IdType="pubmed">33764875</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang C., Chen J., Zong R., Yang S., Fan G. Transformer based scene character detection over low quality images. J. Beijing Univ. Posts Telecommun. 2022;45:124–130.</Citation></Reference><Reference><Citation>Parmar N., Vaswani A., Uszkoreit J., Kaiser L., Shazeer N., Ku A., Tran D. Image transformer; Proceedings of the International Conference on Machine Learning; Stockholm, Sweden. 10–15 July 2018; pp. 4055–4064.</Citation></Reference><Reference><Citation>Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Dehghani M., Minderer M., Heigold G., Gelly S., et al. An image is worth 16 × 16 words: Transformers for image recognition at scale; Proceedings of the International Conference on Learning Representations; Virtual Event, Austria. 3–7 May 2021.</Citation></Reference><Reference><Citation>Wu H., Xiao B., Codella N., Liu M., Dai X., Yuan L., Zhang L. Cvt: Introducing convolutions to vision transformers. arXiv. 20212103.15808</Citation></Reference><Reference><Citation>Xiao T., Singh M., Mintun E., Darrell T., Dollar P., Girshick R. Early convolutions help transformers see better. arXiv. 20212106.14881</Citation></Reference><Reference><Citation>Dong X., Bao J., Chen D., Zhang W., Yu N., Yuan L., Chen D., Guo B. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv. 20212107.00652</Citation></Reference><Reference><Citation>Peng Z., Huang W., Gu S., Xie L., Wang Y., Jiao J., Ye Q. Conformer: Local features coupling global representations for visual recognition. arXiv. 20212105.03889</Citation><ArticleIdList><ArticleId IdType="pubmed">37022836</ArticleId></ArticleIdList></Reference><Reference><Citation>Zhang H., Xu C., Zhang S. Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. arXiv. 20232311.02877</Citation></Reference><Reference><Citation>Zhang H., Zhang S. Shape-IoU: More Accurate Metric considering Bounding Box Shape and Scale. arXiv. 20242312.17663</Citation></Reference><Reference><Citation>Wang C.Y., Yeh I.H., Liao H.Y.M. YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. arXiv. 20242402.13616</Citation></Reference><Reference><Citation>Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C. Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, 11–14 October 2016. Springer International Publishing; Berlin/Heidelberg, Germany: 2016. Ssd: Single shot multibox detector; pp. 21–37. Proceedings, Part I 14.</Citation></Reference><Reference><Citation>Law H., Deng J. Cornernet: Detecting objects as paired keypoints; Proceedings of the European Conference on Computer Vision (ECCV); Munich, Germany. 8–14 September 2018; pp. 734–750.</Citation></Reference></ReferenceList></PubmedData></PubmedArticle>